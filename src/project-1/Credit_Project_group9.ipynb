{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 \n",
    "### Group 9\n",
    "#### Adrian Eriksen, Maren Aamodt, Anette Fredriksen\n",
    "\n",
    "The label in the dataset represent information about persons who has taken out a loan, the response vaiable is if they paid the loan back or not. With 1 beeing they paid back and 2 that they did not. In our project we use action that have values 1 and 0, where 1 is when the money is paid back and 0 is when it is not paid back. Therefore we need to map 2 to 0, this is done in TestLending.py. We assume that the vaiables employment, credit history and other debtors are some of vaiables that have most influence on if the lend money is paid back. \n",
    "\n",
    "#### Program structure\n",
    "\n",
    "fit() fits a Naive Bayes model. \n",
    "\n",
    "set_interest_rate() sets the interest rate.\n",
    "\n",
    "predict_proba() uses the model made in fit() to predict probability for a given x (person).\n",
    "\n",
    "expected_utility() uses predict_proba() to get a prediction probability.\n",
    "\n",
    "get_best_action() uses expected_utility() and checks if the expected return is possitive, if the return is positive a loan is granted. \n",
    "\n",
    "We made a helper function in TestLending.py, mapping. Mapping sets 2 to 0 in the response vaiable (when loan was not payed back). \n",
    "#### Model development\n",
    "\n",
    "We choose Naive Bayes to fit our model, this is done with sklearn using Multinomial Naive Bayes. Naive Bayes is good with classification problems. We did not use action as a parameter in expected_utility, since we used the prediction percentage from fit for is a person pays back. We used the prediction for calculating the probable income or loss. It will be a loss of income if the person does not pay back, so if fit function predicted a low possibility of getting paid back we will not grant a loan. The formula we used is:\n",
    "\n",
    "predicted probability for return * sum of loan * (1 + intrest rate)^months of loan - preicted probability for no return *sum of loan.\n",
    "\n",
    "#### Comparison\n",
    "\n",
    "#### Results for name_banker:\n",
    "\n",
    "Average utility: 1173660.8979637849\n",
    "\n",
    "Average return on investment: 180.21184835525688\n",
    "\n",
    "\n",
    "#### Results for random_banker:\n",
    "\n",
    "Average utility: 566096.2579925589\n",
    "\n",
    "Average return on investment: 83.07326428354087\n",
    "\n",
    "We can se from the result that our model outpreform random_banker by a lot.\n",
    "This is expected since random banker is not considereing any variables when granting loans. \n",
    "Our average return on investment is 216,9% higher than random_banker. \n",
    "\n",
    "## Part 2  \n",
    "### 1.  \n",
    "#### Is it possible to ensure that your policy maximises revenue?  \n",
    "We can never completley assure that our policy maximises revenue, but we can maximise expected revenue. The reason for this is that one can never be sure whether a person will be able to repay a loan.  \n",
    "#### How can you take into account the uncertainty due to the limited and/or biased data?  \n",
    "Taking the uncertainty of our limited and/or biased data into account is hard when we do not have a different dataset to test it up against. If we had this second dataset, we would at least be able to somewhat increase our certainty, based on the fact that we would have information from more than one source. Especially now, during a pandemic, we find it harder to trust data regarding economy and credit worthiness that's collected pre-corona. One way we could deal with this would be to create new data based on the one we posses, but if our original data is inherently flawed, then this might make the model worse. Another way would be to increase the expected utility necessary for granting loans, so that we take fewer risks. We could obviously also increase the interest rate, but in a real world scenario this might just lead to fewer customers and reduce our income.  \n",
    "#### What if you have to decide credit for thousands of individuals and your model is wrong? How should you take that type of risk into account?  \n",
    "First of all, we could start out small with deciding credit for e.g 1000 people. After that we could see how well the model preformed and decide whether to make any adjustments. Another option is to start with only granting loans under a certain amount to make sure that we don't go bankrupt if our model is wrong. We could also create a threshold by finding the standard deviation and mean of expected utility and only grant loans within a given range. \n",
    "\n",
    "Proposed algorithm: We want to have a maximum loan limit, because the loss would be high for large loans if our model does not work. Also, we want to have a minimum expected utility, so that loans with low utility (and smaller chance of the loaner repaying the loan) are rejected. In this way, we minimize the damage if it turns out our model is flawed. Also, as explained above, we limit the number of loaners to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode:\n",
    "nr_of_loaners = 0 #Limiting nr of loaners to 1000 persons\n",
    "\n",
    "if(loan_size < max_loan \n",
    "and expected_utility > min_utility \n",
    "and nr_of_loaners <= 1000):\n",
    "    #<grant loan>\n",
    "    nr_of_loaners += 1\n",
    "else:\n",
    "    #<do not grant loan>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "#### Does the existence of this database raise any privacy concerns?  \n",
    "Yes. First of all, we have the status of their current checking account, credit history and savings accounts. While this in itself might not raise any concerns, one can use their employment status, personal status and sex, property, housing status, age and job to concievably learn who this person is. This may not apply for all people in the database, but a lot of people have little regard for what information they make publicly available through Facebook and other social media. One must also consider the fact that this is the kind of information sought after by hackers, who usually have sophisticated ways of retrieving data from different sources. If the data in this database was made publicly available, it would not be hard for someone with the right skill set to learn the name of most people in the database.  \n",
    "#### If the database was secret (and only known by the bank), but the credit decisions were public, how would that affect privacy?  \n",
    "Making the database only available to the bank would significantly increase the privacy of the data. Making the credit decisions public would not necessarily affect this if there was no other metadata (like the time of the request) published. Given that we have two counters, one for yes and one for no, we find it hard to believe that someone could learn whether a given person is credit worthy based on this alone. That being said, the database only being known to the bank, doesn't necessarily guarantee security. There is always the risk of security breaches, whether it's through phishing attacks or employees selling information. The fact that the numbers are published, does relay the information that we have a database possibly containing valuable information.\n",
    "#### (a) Explain how you would protect the data of the people in the training set.  \n",
    "We have chosen to add laplacian noise to the dataset. (Explanation of laplace here)\n",
    "\n",
    "Another way we could protect the data is by removing unimportant features that does not affect the performance of our utility prediction model. This way, we avoid storing data that could potentially be used to identify our customers. The removal of insignificant attributes could also give a higher level of fairness in our model, because we could avoid having attributes like i.e. gender or age affect the decision of a person getting a loan or not.\n",
    "#### (b) Explain how would protect the data of the people that apply for new loans.  \n",
    "With their lives.  \n",
    "#### (c) Implement a private decision making mechanism for (b), 3 and estimate the amount of loss in utility as you change the privacy guarantee.\n",
    "\n",
    "(We can remove data that the model doesn't use to predict utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
