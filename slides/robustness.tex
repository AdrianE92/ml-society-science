\newcommand{\Bels}{\Xi}
\newcommand \Params {\Theta}
\newcommand \marg {\phi}

% \newcommand \Bay {\ensuremath{\mathscr{B}}}
% \newcommand \Adv {\ensuremath{\mathscr{A}}}


\section{Bayesian inference and privacy}
\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}

\begin{frame}
  \frametitle{Bayesian inference and differential privacy}
  \begin{block}{Bayesian estimation}
    \begin{itemize}
    \item What are its robustness and privacy properties?
    \item How important is the selection of the prior?
    \end{itemize}
  \end{block}

  \begin{block}{Limiting the communication channel}
    \begin{itemize}
    \item How should we communicate information about our posterior?
    \item How much can an adversary learn from our posterior?
    \end{itemize}
  \end{block}

\end{frame}

\subsubsection{Setting}
\begin{frame}
  \begin{block}{Dramatis personae}
    \begin{itemize}
    \item $x$ -- data.
    \item $\Bay$ -- a (Bayesian) statistician.
    \item $\bel$ -- the statistician's prior belief.
    \item $\param$ -- a parameter
    \item $\Adv$ -- an adversary. He knows $\bel$, should not learn $x$.
    \end{itemize}
  \end{block}
  \uncover<2>{
    \begin{block}{The game}
      \begin{enumerate}
      \item \Bay{} selects a model family ($\family$) and a prior
        ($\bel$).
      \item \Bay{} observes data x and calculates the posterior
        $\bel(\param | x)$.
      \item \Adv{} queries  \Bay{}.
      \item \Bay{} responds with a function of the posterior $\bel(\param | x)$.
      \item Goto 3.
      \end{enumerate}
    \end{block}
  }
\end{frame}


\begin{frame}
  \frametitle{Bayesian inference}
  \only<1>{
    \begin{block}{Estimating a coin's bias}
      A fair coin comes heads $50\%$ of the time. 
      We want to test an unknown coin, which we think may not be completely fair. 
    \end{block}
  }
  \only<1,2>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{beta-prior}
      \caption{Prior belief $\bel$ about the coin bias $\theta$.}
    \end{figure}
  }
  \only<2>{
    For a sequence of throws $x_t \in \{0,1\}$,
    \[
    P_\theta(x) \propto \prod_t \theta^{x_t} (1 - \theta)^{1 - x_t}
    = \theta^{\textrm{\#Heads}} (1 - \theta)^{\textrm{\#Tails}}
    \]
  }
  \only<3>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{beta-likelihood}
      \caption{Prior belief $\bel$ about the coin bias $\theta$ and likelihood of $\theta$ for the data.}
    \end{figure}
    Say we throw the coin 100 times and obtain 70 heads. Then we plot the \alert{likelihood} $P_\theta(x)$ of different models.
  }
  \only<4>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{beta-posterior}
      \caption{Prior belief $\bel(\theta)$ about the coin bias $\theta$, likelihood of $\theta$ for the data, and posterior belief $\bel(\theta \mid x)$}
    \end{figure}
    From these, we calculate a \alert{posterior} distribution over the correct models. This represents our conclusion given our prior and the data.
  }
\end{frame}



\begin{frame}
  \frametitle{Bayesian inference}
  \begin{block}{Setting}
    \begin{itemize}
    \item Dataset space $\CS$.
    \item Distribution family $\family \defn \cset{P_\param}{\param \in \Params}$.
    \item Each $P_\param$ is a distribution on $\CS$.
    \item We wish to identify which $\theta$ generated the observed data $x$.
    \item Prior distribution $\bel$ on $\Params$ (i.e. initial belief)
    \item Posterior given data $x \in \CS$ (i.e. conclusion)
      \begin{align}
        \label{eq:posterior}
        \bel(\param \mid x) &= \frac{P_\param(x) \bel(\param)}{\marg(x)}\tag{posterior}\\
        \marg(x) &\defn \sum_{\param \in \Params} P_\param(x) \bel(\param) \tag{marginal}.
      \end{align}
    \end{itemize}
  \end{block}
  \alert{Standard calculation that can be done exactly or approximately.}
\end{frame}




\subsection{Bayesian inference for privacy}
\begin{frame}
  \tableofcontents[hideothersubsections]
\end{frame}


\begin{frame}
  \frametitle{What we want to show}
  \begin{itemize}
  \item<1-> If we assume the family $\family$ is well-behaved \ldots
  \item<1-> \ldots or that the prior $\bel$ is focused on the ``nice'' parts of $\family$
  \item<2-> Inference is robust.
  \item<2-> Our knowledge is private.
  \item<2-> There are also well-known $\family$ satisfying our assumptions.
  \end{itemize}
  \uncover<3>{First, we must generalise differential privacy...}
\end{frame}

\begin{frame}
  \frametitle{Differential privacy of conditional distribution $\bel(\cdot \mid x)$}
  \begin{definition}[$(\epsilon, \delta)$-differential privacy]
    \label{def:dp-old}
    $\bel(\cdot \mid x)$ is \emph{$(\epsilon, \delta)$-differentially private} if, $\forall x \in \CS \alert{= \CX^n}$, $B \subset \Params$
    \[
    \bel(B \mid x) \leq e^\epsilon \bel(B \mid y) + \delta,
    \]
    for all $y$ in the \alert{hamming-$1$ neighbourhood} of $x$. 
  \end{definition}
  \uncover<2->{
    \begin{definition}[$(\epsilon, \delta)$-differential privacy under $\xdistChar$.]
      \label{def:dp-new}
      $\bel(\cdot \mid x)$ is \emph{$(\epsilon, \delta)$-differentially private under } a \alert{pseudo-metric} $\xdistChar : \CS \times \CS \to \Reals_+$ if, $\forall B \subset \Params$ and  $x \in \CS$, 
      \[
      \bel(B \mid x) \leq e^{\epsilon \xdist{x}{y}} \bel(B \mid y) + \delta \xdist{x}{y}, \qquad \forall y \in \CS
      \]
    \end{definition}
    \alert{If two datasets $x,y$ are close, then the distributions $\bel(\cdot \mid x)$ and $\bel(\cdot \mid y)$ are also close.}
  }
\end{frame}


\begin{frame} 
  \frametitle{Sufficient conditions}
  \begin{assumption}[$\family$ is Lipschitz]
    For a given $\rho$ on $\CS$, $\exists L > 0$ s.t. $\forall \param \in \Params$:
    \begin{align}
      \left|\ln \frac{P_\param(x)}{P_\param(y)}\right|
      &\leq
        L \xdist{x}{y}, \qquad \forall x, y \in \mathcal{\CS},
        \label{eq:hoelder-observations}
    \end{align}  
    \label{ass:hoelder-observations}
  \end{assumption}
  \only<1>{
    \includegraphics[width=\textwidth]{beta-lipschitz}
  }
  \only<2>{
    \includegraphics[width=\textwidth]{beta-log-lipschitz}
  }
\end{frame}


\begin{frame}
  \frametitle{Stochastic Lipschitz condition}
  \begin{assumption}[The prior is concentrated on nice parts of $\family$]
    Let the set of $L$-Lipschitz parameters be $\Theta_L$.
    Then $\exists c \constg > 0$ s.t. 
    \begin{equation}
      \bel(\Params_L) \geq 1 - \constg \exp(-cL), \forall L
    \end{equation}
    \label{ass:hoelder-measure-observations}
  \end{assumption}
  \only<1>{
    \includegraphics[width=\textwidth]{beta-log-lipschitz}
  }
  \only<2>{
    \includegraphics[width=\textwidth]{beta-log-lipschitz-prior}
  }
\end{frame}


\subsection{Robustness and privacy of the posterior distribution}
\label{sec:robustness}
\begin{frame}
  \frametitle{Robustness of the posterior distribution}
  \begin{definition}[KL divergence]
    \begin{equation}
      \label{eq:kl-divergence}
      \dist{P}{Q} \defn \int \ln \frac{\dd{P}}{\dd{Q}} \dd{P}.
    \end{equation}
  \end{definition}
  \begin{theorem}
    \begin{enumerate}[(i)]
    \item<2-> Under Assumption~\ref{ass:hoelder-observations},
      \begin{equation}
        \dist{\bel(\cdot \mid x)}{\bel(\cdot \mid y)}
        \leq
        2L\xdist{x}{y}
      \end{equation}
      \label{the:kl-1}
    \item<3-> Under Assumption~\ref{ass:hoelder-measure-observations},
      \begin{equation}
        \dist{\bel(\cdot \mid x)}{\bel(\cdot \mid y)}
        \leq
        \frac{\kappa C_\bel }{c} \cdot 
        \xdist{x}{y}
      \end{equation}
      \label{the:kl-2}
    \end{enumerate}
    \label{the:kl}
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{block}{Differential privacy of the posterior distribution}
    \begin{itemize}
    \item Under Assumption~\ref{ass:hoelder-observations}, 
      $B \in \fields{\Params}$:
      \begin{align}
        \bel(B \mid x) & \leq e^{ 2L \xdist{x}{y}} \bel(B \mid y) 
      \end{align}
      \ie the posterior is $(2L, 0)$-DP under $\xdistChar$.
      \label{thm:dp1}
    \item Under Assumption~\ref{ass:hoelder-measure-observations}, for
      all $x,y \in \mathcal{\CS}$, $B \in \fields{\Params}$:
      \begin{align*}
        \abs{\bel(B \mid x) - \bel(B \mid y)}
        & \leq \sqrt{\frac{\constScaleB C_\bel }{2c} \xdist{x}{y}}
      \end{align*}
      \ie the posterior is $\left(0,  \sqrt{\frac{\constScaleB C_\bel }{2c}}\right)$-DP under $\sqrt{\xdistChar}$.
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Posterior sampling query model}

\begin{frame}
  \frametitle{Posterior sampling query model}
  \begin{itemize}
  \item<1-> We select a prior $\bel$.
  \item<1-> We observe data $x$.
  \item<1-> We calculate a posterior $\bel(\cdot \mid x)$.
  \item<1-> An adversary has sampling-based access to the posterior.
  \end{itemize}

  \uncover<2->{
    \begin{block}{First idea}
      At time $t$, the adversary observes a \alert{sample} from the posterior:
      \[
      \param_t \sim \bel(\param \mid x),
      \]
    \end{block}
  }
  \uncover<3->{
    $\Adv$ may instead \alert{query} using a function $q : \Theta \to \CR$, to obtain:
    \[
    r_t = q(\theta_t)
    \]
  }
\end{frame}

\begin{frame}
  \frametitle{Responding to queries via utilities}
  \begin{block}{Posterior sampling}
    Given a prior $\bel$, data $x$ and number of samples $n$,
    \[
    \hat{\Params} \sim \bel^n( \cdot \mid x).
    \]
  \end{block}
  \begin{block}{Sample query response}
    For a query $q_t$ and utility function $u_\param : \CR \times \CQ \to [0,1]$, return:
    \[
    r_t \in \argmax_r \sum_{\param \in \hat{\Params}} u_\param(r, q_t)
    \]
  \end{block}
  \begin{theorem}
    If $\bel^\star$ is \Adv{}'s preferred prior, and we restrict it so  $\bel(\Params_L) = 1$:
    \begin{enumerate}[(a)]
    \item The algorithm is $2 L n$-differentially private.
    \item \Adv{}'s regret is  $O([1 - \bel^\star(\Params_L)] + \sqrt{\ln (1/\delta)/n})$, w.p. $1 - \delta$.
    \end{enumerate}
  \end{theorem}
\end{frame}

\begin{frame}
  \frametitle{Another look at the exponential mechanism}
  Define a utility function $u(x,r)$
  \[
  p(r) \propto e^{\epsilon u(x,r)} \mu(r).
  \]
  Respond with $r$ with probability $p(r)$.


  \begin{block}{Connection to posterior mechanism}
    \begin{itemize}
    \item Responses are parameters $\theta$.
    \item Take $u(\theta,x) = \log P_\theta(x)$.
    \item Take $\mu(\theta) = \bel(\theta)$.
    \item Then $p(\theta) = \bel(\theta \mid x)$.
    \item Rather than tuning $\epsilon$, we can tune 
      \begin{itemize}
      \item The prior $\bel$.
      \item The number of samples $n$.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}




\subsection{Experiments}
\begin{frame}
  \includegraphics[width=0.95\linewidth]{figures/DNB_16_var-accuracy-eps}
\end{frame}

\begin{frame}
  \includegraphics[width=0.95\linewidth]{figures/lr-mse-samplerate.pdf}
\end{frame}


\begin{frame}
  \frametitle{Multi-armed bandits}
  \includegraphics[width=\textwidth]{figures/arms_10_eps_01}

\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  \begin{itemize}
  \item Bayesian inference is inherently robust and private [hooray].
  \item Privacy is achieved by posterior sampling [Dimitrakakis et al].
  \item In certain cases by parameter noise [Zhang et al].
  \item DP also applicable to bandits [Tossou and Dimitrakakis] - Open problem: Thompson sampling.
  \item How to tune for unknown constants? (General problem in DP)
  \end{itemize}
  {\tiny
    \begin{block}{References}
      \begin{itemize}
      \item C Dwork, F McSherry, K Nissim, A Smith, \emph{Calibrating  noise to sensitivity in private data analysis}, TCC 2006.
      \item C. Dimitrakakis, B. Nelson, A. Mitrokotsa, B. Rubinstein, \emph{Robust and Private Bayesian Inference}, ALT 2014. 
      \item A. Tossou, C. Dimitrakakis, \emph{Algorithms for differentially private multi-armed bandits}, AAAI 2016.
      \item D. Mir, \emph{Information-theoretic foundations of
          differential privacy}, EDBT/ICDT, 2012.
      \item YX. Wang, SE. Fienberg, A. Smola, \emph{Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo}, ICML 2015.
      \item Z. Zhang, B. Rubinstein, C. Dimitrakakis, \emph{On the Differential Privacy of Bayesian Inference}, AAAI 2016.
      \end{itemize}
    \end{block}
  }
\end{frame}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "privacy_presentation.tex"
%%% End: 
