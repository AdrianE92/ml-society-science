
\section{Introduction}

\only<article>{One of the most important methods in machine learning
  and statistics is that of Bayesian inference.  This is the most
  fundamental method of drawing conclusions from data and explicit
  prior assumptions. In Bayesian inference, prior assumptions are
  represented as a probabilities on a space of hypothesis. Each
  hypothesis is seen as a probabilistic model of all possible data
  that we can see.}

\section{The basics of Bayesian inference}
\only<article>{Frequently, we want to draw conclusions from data. However, the conclusions are never solely inferred from data, but also depend on prior assumptions about reality. 


  \begin{example}
    John claims he has psychic powers and can predict a series of coin tosses. We oblige, and throw a coin 8 times.  John predicts 8 out of 8 coin tosses. The probability of him doing so by chance is $2^{-8}$. If he was a medium, as he claims, then his probability of achieving the feat would be $1$. Should we believe John?
  \end{example}


  \begin{example}
    Traces of DNA are found at a murder scene. We perform a DNA test against a database of $10^4$ citizens registered to be living in the area. We know that the probability of a false positive (that is, the test finding a match by mistake) is $10^{-6}$. If there is a match in the database, does that mean that the citizen was at the scene of the crime?
  \end{example}

  Answering these questions requires us to clearly define what are the possible hypotheses we wish to consider. Taking the first example, we can define two:
  \begin{enumerate}
  \item hypothesis $M$, that John is a medium.
  \item hypothesis $\neg M$, that John is not a medium.
  \end{enumerate}
  We can also define a probability model for the number of successful predictions that John would make in either case. 

  Let $x_t$ be $0$ if John makes an incorrect prediction at time $t$ and $x_t = 1$ if he makes a correct prediction. John's claim that he can predict our tosses perfectly means that for a sequence of tosses $\bx = x_1, \ldots, x_n$,
  \[
  \Pr(\bx \mid M) = \begin{cases}
    1, & x_t = 1 \forall t \in [n]\\
    0, & \exists t \in [n] : x_t = 0.
  \end{cases}
  \]
  That is, the probability of perfectly correct predictions is 1, and that of one or more incorrect prediction is 0. For the other model, we can assume that all draws are independently and identically distributed from a fair coin. Consequently, no matter what John's predictions are, we have that:
  \[
  \Pr(\bx \mid \neg M) = 2^{-n}.
  \]
  So, for the given example, as stated, we have the following facts:
  \begin{itemize}
  \item If John makes one or more mistakes, then $\Pr(\bx \mid M) = 0$ and $\Pr(\bx \mid \neg M) = 2^{-n}$. Thus, we should perhaps say that then John is not a medium
  \item If John makes no mistakes at all, then 
    \begin{align}
      \Pr(\bx \mid M) &= 1,
      &
        \Pr(\bx \mid \neg M) &= 2^{-n}.
    \end{align}
  \end{itemize}
  Does that mean that we must conclude that John is a medium? What if
  $n = 1$? What if $n=100$?  In fact, our conclusion should somehow
  depend on the strength of the evidence. Should it also not depend on how
  likely we think that a medium exists?

  It is this latter idea that we'll try and exploit. We'd like to combine the weight of the evidence, with the weight of our prior beliefs about reality.
  To do this, we first recall the definition of conditional probability. 
}
\subsection{Conditional probability and Bayesian inference}
\only<article>{
  Let $A$ and $B$ be two events. Let $P(A)$ be the probability of event $A$ and $P(B)$ the probability of event $B$. We can think of the probability of an event as the \emph{relative size} of the event in the space of probabilities.\footnote{More formally, probability is a measure; a function similar to volume, area, length and mass.} 
  The probability of both events $A$ and $B$ happening at the same time is denoted by $P(A \cap B)$.  This amounts to measuring the size of the space by the intersection of $A$ and $B$.
  The basic probability laws are the following.
}
\begin{block}{Axioms of probability}
  \begin{enumerate}
  \item The probability of the certain event is $P(\Omega) = 1$
  \item The probability of the impossible event is $P(\emptyset) = 0$
  \item The probability of any event $A$ is $1 \leq P(A) \geq 0$.
  \item If $A, B$ are disjoint, i.e. $A \cap B = \emptyset$, meaning
    that they cannot happen at the same time, then
    \[
    P(A \cup B) = P(A) + P(B)
    \]
  \end{enumerate}
\end{block}


\only<article>{
  Sometimes we would like to calculate the probability of some event $A$ happening given that we know that some other event $B$ has happened. For this we need to first define the idea of conditional probability.
}
\begin{definition}[Conditional probability]
  The probability of $A$ happening if we know that $B$ has happened is defined to be:
  \[
  P(A \mid B) \defn \frac{P(A \cap B) }{P(B)}.
  \]
\end{definition}
\only<article>{
  Here, the probability measure of any event $A$ given $B$ is defined to be the probability of the intersection of of the events divided by the second event.
  We can rewrite this definition as follows, by using the definition for $P(B \mid A)$}
\begin{block}{Bayes's theorem}
  \[
  P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.
  \]
\end{block}
\only<article>{
  Now let us apply this idea to our specific problem. This allows us to calculate the probability of John being a medium, given the data:
  \[
  \Pr(M \mid \bx) = \frac{\Pr(\bx \mid M) \Pr(M)}{\Pr(\bx)},
  \]
  where
  \[
  \Pr(\bx) = \Pr(\bx \mid M) \Pr(M) + \Pr(\bx \mid \neg M) \Pr(\neg M).
  \]
  The only thing left to specify is $\Pr(M)$, the probability that John is a medium before seeing the data. This is our subjective prior belief that mediums exist and that John is one of them. 

  More generally, we can think of Bayesian inference as follows: 
  \begin{itemize}
  \item We start with a set of mutually exclusive hypotheses $H = \{M_1, \ldots, M_k\}$.
  \item Each hypothesis $M$ is represented by a specific probabilistic model for any possible data $\bx$, that is $\Pr(\bx \mid M)$.
  \item For each hypothesis, we have a prior probability $\Pr(M)$ that it is correct.
  \item After observing the data, we can calculate a posterior probability that the hypothesis is correct:
    \[
    \Pr(M \mid \bx) = \frac{\Pr(\bx \mid M) \Pr(M)}{\sum_{i=1}^k \Pr(\bx \mid M_i) \Pr(M_i)}.
    \]
  \end{itemize}
  Combining the prior belief with evidence is key in this procedure. Our posterior belief can then be used as a new prior belief when we get more evidence. 
}

\section{Naive Bayes classifiers}
\only<article>{
  One special case of this idea is in classification, when each hypothesis corresponds to a specific class. Then, given a new example vector of data $\bx$, we would like to calculate the probability of different classes $C$ given the data, $\Pr(C \mid \bx)$.

  From Bayes's theorem, we see that we can write this as
}
\[
\Pr(C \mid \bx) = \frac{\Pr(\bx \mid C) \Pr(C)}{\sum_{i} \Pr(\bx \mid C_i) \Pr(C_i)}
\]
\only<article>{
  for any class $C$. This directly gives us a method for classifying new data, as long as we have a way to obtain $\Pr(\bx \mid C)$ and $\Pr(C)$.
}
\begin{block}{Calculating the prior probability of classes}
  A simple method is to simply count the number of times each class appears in the training data $\Training = ((x_t, y_t))_{t=1}^T$. Then we can set 
  \[
  \Pr(C) = 1/T \sum_{t=1}^T \ind{y_t = C}
  \]
\end{block}

\only<article>{
  The Naive Bayes classifier uses the following model for observations, where observations are independent of each other given the class. Thus, for example the result of three different tests for lung cancer (stethoscope, radiography and biopsy) only depend on whether you have cancer, and not on each other.
}
\begin{block}{Probability model for observations}
  \[
  \Pr(\bx \mid C) =  \Pr(x(1), \ldots, x(n) \mid C) =
  \prod_{k=1}^n \Pr(x(k) \mid C).
  \]
\end{block}

\only<article>{There are two different types of models we can have, one of which is mostly useful for continuous attributes and the other for discrete attributes.
  In the first, we just need to count the number of times each feature takes different values in different classes.
}
\begin{block}{Discrete attribute model.}
  \only<article>{Here we simply count the average number of times that the attribute $k$ had the value $i$ when the label was $C$. This is in fact analogous to the conditional probability definition.}
  \[
  \Pr(x(k) = i \mid C) 
  =
  \frac{\sum_{t=1}^T \ind{x_t(k) = i \wedge y_t = C}}
  {\sum_{t=1}^T \ind{y_t = C}}
  = 
  \frac{N_k(i, C)}{N(C)},
  \]
  where $N_k(i, C)$ is the numb l l .er of examples in class $C$ whose $k$-th attribute has the value $i$, and $N(C)$ is the number of examples in class $C$.
\end{block}

\only<article>{
  Sometimes we need to be able to deal with cases where there are no examples at all of one class. In that case, that class would have probability zero. To get around this problem, we add ``fake observations'' to our data. This is called \emph{Laplace smoothing}.
  \begin{remark} In Laplace smoothing with constant $\lambda$, our probability model is
    \[
    \Pr(x(k) = i \mid C) 
    =
    \frac{\sum_{t=1}^T \ind{x_t(k) = i \wedge y_t = C} + \lambda}
    {\sum_{t=1}^T \ind{y_t = C} + n_k \lambda}
    = 
    \frac{N_k(i, C) + \lambda}{N(C) + n_k \lambda}.
    \]
    where $n_k$ is the number of values that the $k$-th attribute can take. This is necessary, because we want $\sum_{i=1}^{n_k} \Pr(x(k) = i \mid C) = 1$. (You can check that this is indeed the case as a simple exercise).
  \end{remark}

  \begin{remark}
    In fact, the Laplace smoothing model corresponds to a so-called Dirichelt prior over polynomial parameters with a marginal probability of observation equal to the Laplace smoothing.
  \end{remark}
}



\begin{block}{Continuous attribute model.}
  \only<article>{Here we can use a Gaussian model for each continuous dimension.}
  \[
  \Pr(x(k) = v \mid C) 
  =
  \frac{1}{\sigma \sqrt{2 \pi}} e^{\frac{(v - \mu)^2}{\sigma^2}},
  \]
  where $\mu$ and $\sigma$ are the mean and variance of the Gaussian, typically calculated from the training data as:
  \begin{align*}
    \mu &=   \frac{\sum_{t=1}^T x_t(k) \ind{y_t = C}}
          {\sum_{t=1}^T \ind{y_t = C}},
  \end{align*}
  i.e. $\mu$ is the mean of the $k$-th attribute when the label is $C$ and
  \begin{align*}
    \sigma &=   \frac{\sum_{t=1}^T [x_t(k) - \mu]^2 \ind{y_t = C}}
             {\sum_{t=1}^T \ind{y_t = C}},
  \end{align*}
  i.e. $\sigma$ is the variance of the $k$-th attribute when the label is $C$.
  Sometimes we can just fix $\sigma$ to a constant value, i.e. $\sigma = 1$.
\end{block}

\begin{alertblock}{Estimates versus true probabilities}
  Remember that the probabilities we get from this calculation are only \alert{estimates}. We do not really know the probabilities of each observation given the classes: we are only estimating them from the data. It is also possible that our assumption about the independence of features is completely wrong.
\end{alertblock}
\begin{exercise}
  This is an exercise to get you familiar with the NaiveBayes implementation in the \texttt{e1071} library.
  \begin{itemize}
  \item First, open R and install the library \texttt{e1071} by doing
\begin{verbatim}
      > install.packages('e1071', dependencies = TRUE)
\end{verbatim}
  \item Then load the library:
\begin{verbatim}
\library('e1071')
\end{verbatim}
  \item Then, check the documentation either by going to
    \url{http://www.inside-r.org/packages/cran/e1071/docs/naivebayes}
    or by simply typing
\begin{verbatim}
> ?naiveBayes
\end{verbatim}
    in R.
  \end{itemize}

  then type the commands in the tutorial. Some explanations are given below.
\end{exercise}


The following line creates a Naive Bayes model predicting the \texttt{Class} variable from all the other variables.
\begin{lstlisting}
model <- naiveBayes(Class ~ ., data = Training)
\end{lstlisting}

There are two ways to predict new data given our model. The first 
method gives us the labels as outputs
\begin{lstlisting}
class.predictions <- predict(model, Holdout ) 
\end{lstlisting}
The second method gives us the class probabilities as outputs
\begin{lstlisting}
prob.predictions <- predict(model, Holdout, type = "raw") # 
\end{lstlisting}

We can create a contingency table from our class predictions
\begin{lstlisting}
table(class.predictions, Y$Class) 
\end{lstlisting}
More precisely, the command
\begin{lstlisting}
 A <- table(x, y)
\end{lstlisting}
gives a matrix, whose entry $A_{ij}$ is equal to the number of times
that $x_t = i$ and $y_t = j$. Consequently, the sum of terms in the diagonal is the number of correctly classified examples and the sum of the remaining terms it that of the incorrectly classified examples.


\begin{exercise}
  The purpose of this exercise is to explore the effect of the Laplace smoothing parameter $\lambda$ on Naive Bayes classification. For this exercise use the package DNA:
\begin{lstlisting}
data(DNA, package = "mlbench")
\end{lstlisting}
  The class labels are stored in the column \texttt{Class}. Use
  \begin{itemize}
  \item Data points 1--1593 for training (store it in a variable called \texttt{Training})
  \item Data points 1594--2124 for holdout (store it in a variable called \texttt{Holdout})
  \item Data point 2125--3186 for testing  (store it in a variable called \texttt{Testing})
  \end{itemize}
  Then do the following:
  \begin{enumerate}
  \item Use a loop to go through the parameters $\lambda  \in \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}$ and for each value
    \begin{itemize}
    \item Train a model using 
\begin{lstlisting}
model <- naiveBayes(Class ~ ., data = Training, laplace = lambda)
\end{lstlisting}
    \item Measure the classification error of the model on the Holdout data using \texttt{predict} and do a plot for all different lambda, using a \texttt{for} loop.
    \item Save the plot in a PDF file using the \texttt{pdf} command.
    \item Find the best value (with lowest error) for $\lambda$ on the Holdout set.
    \item Test the accuracy of the model with the best $\lambda$ on the Test set.
    \end{itemize}
  \end{enumerate}
  Then submit the following items \emph{with a private message on Piazza, using the tag [hw4]}.
  \begin{enumerate}
  \item Your R code, in a single R file named \texttt{MyNameBayes.R} which I should be able to directly run with \texttt{source("MyNameBayes.R")}; to produce:
    \begin{enumerate}[(a)]
    \item The hold out classification error for the different
      lambdas, in a plot. You can generate PDF plots with the \texttt{pdf} command (see also my example: \texttt{knnExample.R})
    \item The value of the best $\lambda$, and the
      resulting testing error classification.
    \end{enumerate}
  \item An answer to the following questions:
    \begin{enumerate}[(a)]
    \item How does the testing error compare to the training and holdout
      error? Why do you think this is the case?
    \item What do you think are the advantages and disadvanages of Naive
      Bayes over KNN and decision trees?
    \end{enumerate}
  \end{enumerate}
\end{exercise}

\subsection{Full Bayesian inference}
\only<article>{Statistical inference is the problem estimating quantities from data. Many machine learning problems lie within the statistical inference framework. While there are many paradigms within this framework, one typically tries to calculate probability distributions of some random variables from data. These are usually expressed in terms of parametrised distributions. In the Bayesian setting in particular, the goal is to quantify our uncertainty about these unknown parameters in terms of another probability distribution.}
\begin{frame}
  \frametitle{Bayesian inference} \only<article>{ Bayesian statistical
    inference is the problem of inferring distributions (of
    parameters) from data. Typically, given some prior distribution
    $\bel(\param)$ on parameters $\param \in \Param$, such that for
    every possible parameter value the probability $P_\param(x)$ is
    well defined for any $x \in \CX$, we wish to infer a posterior
    distribution from any given data $x$.}
  \begin{definition}[Posterior distribution]
    \only<article>{Given a prior $\bel$ on the parameter set $\Param$
      and a family $\{P_\param\}$ of distributions on $\CX$, the
      posterior distribution of parameter $\param$ for any data
      $x \in \CX$ is}
    \begin{equation}
      \bel(\param \mid x) \defn \frac{P_\param(x) \bel(\param)}{\sum_{\param' \in \Param}
        P_{\param'}(x) \bel(\param')}.
      \label{eq:posterior}
    \end{equation}
  \end{definition}
\end{frame}




\subsection{Bayesian networks}
\only<article>{
  A Bayesian network is a collection of $n$ random variables $x_i : \Omega \to X_i$, and let $X \defn \prod_i X_i$, with underlying probability measure $P$ on $\Omega$.
  Let $\bx = (x_i)_{i=1}^n$ and for any subset $B \subset[n]$ let
  \begin{align}
    \bx_B &\defn (x_i)_{i \in B}\\
    \bx_{-j} &\defn (x_i)_{i \neq i}
  \end{align}
}
\begin{frame}
  \frametitle{Bayesian networks and independence}
  \begin{block}{Joint probability}
    Let $\bx = (x_1, \ldots, x_n)$. Then $\bx : \Omega \to X$, $X = \prod_i X_i$ and:
    \[
      \Pr(\bx \in A) = P(\cset{\omega \in \Omega}{\bx(\omega) \in A}).
    \]
  \end{block}
  \only<article>{
    When $X_i$ are finite, we can typically write
    \[
      \Pr(\bx = \ba) = P(\cset{\omega \in \Omega}{\bx(\omega) = \ba}),
    \]
    for the probability that $x_i = a_i$ for all $i \in [n]$.
  }
  \begin{block}{Factorisation}
    \only<article>{
      For any subsets $B \subset [n]$ and its complement $C$ so that
      $\bx_B = (x_i)_{i \in B}$,     $\bx_C = (x_i)_{i \notin B}$
    }
    \[
      \Pr(\bx) = \Pr(\bx_B \mid \bx_C) \Pr(\bx_C)
    \]
  \end{block}
  \begin{block}{Conditional independence}
    We say $x_i$ is conditionally independent of $\bx_B$ given $\bx_D$ and write $x_i \mid \bx_D \indep \bx_B$ iff
    \[
      \Pr(x_i, \bx_B \mid \bx_D)
      =
      \Pr(x_i \mid \bx_D)
      \Pr(\bx_D \mid \bx_B).
    \]
  \end{block}
  \only<article>{In a Bayesian network, conditional independence is represented through directed edges.}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_i$};
      \node[RV] at (0,0) (xB) {$x_j$};
      \node[RV] at (1,0) (xD) {$x_k$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}
    \label{fig:bn}
    \caption{Bayesian network exhibiting the factorisation $\Pr(x_i \mid x_j) \Pr(x_j \mid x_k)$.}
  \end{figure}
\end{frame}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: t
%%% End:
