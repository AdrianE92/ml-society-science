\section{Nearest neighbours}
\begin{frame}
  \frametitle{Discriminating between diseases}
  \input{../figures/KNN3.tikz}
\end{frame}

\only<article>{
  Let's tackle the problem of discriminating between different
  disease vectors. Ideally, we'd like to have a simple test that
  tells us what ails us. One kind of test is mass spectrometry. This
  graph shows spectrometry results for two types of bacteria. There
  is plenty of variation within each type, both due to measurement
  error and due to changes in the bacterial strains. Here, we plot
  the average and maximum energies measured for about 100 different
  examples from each strain.
}

\begin{frame}
  \frametitle{Nearest neighbour: the hidden secret of machine learning}
  \input{../figures/separation1.tikz}
\end{frame}

\only<article>{
  Now, is it possible to identify an unknown strain based on this
  data? Actually, this is possible. Sometimes, very simple algorithms
  work very well. One of the simplest one involves just measuring the
  distance between the decsription of a new unknown strain and known
  ones. In this visualisation, I projected the 1300-dimensional data
  into a 2-dimensional space. Here you can clearly see that it is
  possible to separate the two strains. In order to classify a new
  point, you just need to see whether it's closer to the train VVT or
  BUT.
}

\begin{frame}
  \frametitle{Comparing spectral data}
  \only<1>{\input{../figures/difference1.tikz}}
  \only<presentation>{\only<2>{\input{../figures/difference2.tikz}}}
\end{frame}

\only<article>{
  The choice of distance in this kind of algorithm is important,
  particularly for very high dimensions. For something like a
  spectrogram, one idea is look at the total area of the difference
  between two spectral lines. 
}

\begin{frame}
  \frametitle{The nearest neighbour algorithm}
  \begin{algorithm}[H]
    \begin{algorithmic}
      \State \textbf{Input} Data $D = \{(x_1, y_1), \ldots, (x_T, y_T)\}$, neighbourhood $k \geq 1$, distance $d : \CX \times \CX \to \Reals_+$, new point $x \in \CX$
      \State $D = \texttt{Sort}(D, d)$ \% \textsf{ Sort $D$ so that $d(x, x_i) \leq d(x, x_{i+1})$}.
      \State $p_y = \sum_{i=1}^k \ind{y_i = y} / k$ for $y \in \CY$.
      \State \textbf{Return} $\argmax_y p_y$
    \end{algorithmic}
    \caption{$k$-NN Classify}
    \label{alg:kNN-classify}
  \end{algorithm}
  \only<article>{The nearest neighbour algorithm for classification (Alg.~\ref{alg:kNN-classify}) does not include any complicated learning. Given a training dataset $D$, it returns a classification decision for any new point $x$ by simply comparing it to its closest $k$ neighbours in the dataset. It then estimates the probability $p_y$ of each class $y$ by calculating the average number of times the neighbours take the class $y$.
  }
  \only<presentation>{
    What does the algorithm output when $k = T$?
  }
\end{frame}

\begin{frame}
  \frametitle{Nearest neighbour: What type is the new bacterium?}
  \input{../figures/separation2.tikz}
  \only<presentation>{
    \only<2>{\Large \alert{What if it a \textbf{completely different strain}?}}
  }
  \only<article>{Given that the $+$ points represent the BUT type, and the $\times$ points the VVJ type, what type of bacterium could the circle point be?}
\end{frame}

\only<presentation>{
  \begin{frame}
    \frametitle{Hands on with Python console}
    \hyperlink{../src/decision-problems/knn-classify.py}{\beamerbutton{KNN example}}
    %% Use knn-classify to simply demonstrate a kNN classifier.
  \end{frame}
}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes.tex"
%%% End:
