\section{Nearest neighbours}
\begin{frame}
  \frametitle{Discriminating between diseases}
  \input{../figures/KNN3.tikz}
\end{frame}

\only<article>{
  Let's tackle the problem of discriminating between different
  disease vectors. Ideally, we'd like to have a simple test that
  tells us what ails us. One kind of test is mass spectrometry. This
  graph shows spectrometry results for two types of bacteria. There
  is plenty of variation within each type, both due to measurement
  error and due to changes in the bacterial strains. Here, we plot
  the average and maximum energies measured for about 100 different
  examples from each strain.
}

\begin{frame}
  \frametitle{Nearest neighbour: the hidden secret of machine learning}
  \input{../figures/separation1.tikz}
\end{frame}

\only<article>{ Now, is it possible to identify an unknown strain
  based on this data? Actually, this is possible. Sometimes, very
  simple algorithms work very well. One of the simplest one involves
  just measuring the distance between the decsription of a new unknown
  strain and known ones. In this visualisation, I projected the
  1300-dimensional data into a 2-dimensional space. Here you can
  clearly see that it is possible to separate the two strains. We can
  use the distance to examples VVT and BUT in order to decide the type
  of an unknown strain.  }

\begin{frame}
  \frametitle{Comparing spectral data}
  \only<1>{\input{../figures/difference1.tikz}}
  \only<presentation>{\only<2>{\input{../figures/difference2.tikz}}}
\end{frame}

\only<article>{
  The choice of distance in this kind of algorithm is important,
  particularly for very high dimensions. For something like a
  spectrogram, one idea is look at the total area of the difference
  between two spectral lines. 
}

\begin{frame}
  \frametitle{The nearest neighbour algorithm}
  \only<article>{The nearest neighbour algorithm for classification (Alg.~\ref{alg:kNN-classify}) does not include any complicated learning. Given a training dataset $D$, it returns a classification decision for any new point $x$ by simply comparing it to its closest $k$ neighbours in the dataset. It then estimates the probability $p_y$ of each class $y$ by calculating the average number of times the neighbours take the class $y$.
  }
  \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \State \textbf{Input} Data $D = \{(x_1, y_1), \ldots, (x_T, y_T)\}$, $k \geq 1$,  $d : \CX \times \CX \to \Reals_+$, new point $x \in \CX$
      \State $D = \texttt{Sort}(D, d)$ \% \textsf{ Sort $D$ so that $d(x, x_i) \leq d(x, x_{i+1})$}.
      \State $p_y = \sum_{i=1}^k \ind{y_i = y} / k$ for $y \in \CY$.
      \State \textbf{Return} $\vp \defn (p_1, \ldots, p_k)$
    \end{algorithmic}
    \caption{\KNN{} Classify}
    \label{alg:kNN-classify}
  \end{algorithm}
  \begin{alertblock}{Algorithm parameters}
    \only<article>{In order to use the algorithm, we must specify some parameters, namely.}
    \begin{itemize}
    \item Neighbourhood $k \geq 1$. \only<article>{The number of neighbours to consider.}
    \item Distance $d : \CX \times \CX \to \Reals_+$. \only<article>{The function we use to determine what is a neighbour.}
    \end{itemize}
  \end{alertblock}
  \only<presentation>{
    What does the algorithm output when $k = T$?
  }
\end{frame}

\begin{frame}
  \frametitle{Nearest neighbour: What type is the new bacterium?}
  \input{../figures/separation2.tikz}
  \only<presentation>{
    \only<2>{\Large \alert{What if it a \textbf{completely different strain}?}}
  }
  \only<article>{Given that the $+$ points represent the BUT type, and the $\times$ points the VVJ type, what type of bacterium could the circle point be?}
\end{frame}

\begin{frame}
  \frametitle{Separating the model from the classification policy}
  \begin{itemize}
  \item The \KNN{} algorithm returns a model giving class probabilities for new data points.
  \item \only<article>{It is up to us to decide how to use this model to decide upon a given class. A typical decision making rule can be in the form of a policy $\pol$ that depends on what the model says. However, the simplest decision rule is to take the most likely class:}
    \only<presentation>{Deciding a class given the model}
    \[
    \pol(a \mid x) = \ind{p_a \geq p_y \forall y}, \qquad \vp = \KNN(D, k, d, x)
    \]
  \end{itemize}
\end{frame}

\only<presentation>{
  \begin{frame}
    \frametitle{Hands on with Python console}
    \hyperlink{../src/decision-problems/knn-classify.py}{\beamerbutton{KNN example}}
    %% Use knn-classify to simply demonstrate a kNN classifier.
  \end{frame}
}

\begin{frame}
  \frametitle{Discussion: Shortcomings of $k$-nearest neighbour}
  \begin{itemize}
  \item Choice of $k$
  \item Choice of metric.
  \item Representation of uncertainty.
  \item Scaling with large amounts of data.
  \end{itemize}
  \only<article>{For that reason, it is best to think of \KNN{} as a \alert{model} for predicting the class of a new example from a finite set of existing classes. The model itself might be incorrect, but this should nevertheless be OK for our purposes. In particular, we might later use the model in order to derive classification rules.}

\end{frame}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes.tex"
%%% End:
